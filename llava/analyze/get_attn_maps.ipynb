{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP encodes the image once, hence each layer has one attention map\n",
    "# LLaMA encodes the sentence multiple times, and with only queries for generation,\n",
    "# hence the latter attention maps of the same layer\n",
    "# have the shape [B, H, 1, KV_LEN]\n",
    "# Hence we create once attention matrix for one layer by concating everything\n",
    "# `rollout(...)` copied over from \n",
    "# https://github.com/sayakpaul/vit-explain/blob/4f92628ed4b5109f43febd2976f688e585baa44b/vit_rollout.py\n",
    "# Thanks @sayakpaul!\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import cv2\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "\n",
    "def calculate_modality_indices(inputs_emb_modalities, bsz=1, seq_len=None):\n",
    "    # bsz, num_heads, q_len, kv_len = attn_weights.size()\n",
    "    \n",
    "    if seq_len == None:\n",
    "        raise ValueError(\"`seq_len` should be a positive integer, found None.\")\n",
    "    \n",
    "    image_attn_mask = torch.zeros(bsz, seq_len)\n",
    "    video_attn_mask = torch.zeros(bsz, seq_len)\n",
    "    text_attn_mask = torch.zeros(bsz, seq_len)\n",
    "    \n",
    "    mask_map = dict(\n",
    "        text=text_attn_mask,\n",
    "        image=image_attn_mask,\n",
    "        video=video_attn_mask\n",
    "    )\n",
    "    \n",
    "    modalities_buffer = inputs_emb_modalities\n",
    "    # List[List[Dict['modality': num_tokens]]]\n",
    "    \n",
    "    for example_idx in range(len(modalities_buffer)):\n",
    "        example_buffer = modalities_buffer[example_idx]\n",
    "        running_tok_idx = 0\n",
    "        for chunk_idx in range(len(example_buffer)):\n",
    "            chunk_modality = list(example_buffer[chunk_idx].keys())[0]\n",
    "            chunk_tokens = list(example_buffer[chunk_idx].values())[0]\n",
    "            mask_map[chunk_modality][example_idx, running_tok_idx : running_tok_idx + chunk_tokens] = 1\n",
    "            running_tok_idx += chunk_tokens\n",
    "    \n",
    "    \n",
    "    return image_attn_mask, video_attn_mask, text_attn_mask\n",
    "\n",
    "def combine_attention(layer_attn_list):\n",
    "    final_attn = list()\n",
    "    max_len = layer_attn_list[-1].shape[-1]\n",
    "    \n",
    "    num_generated_tokens = 0\n",
    "    \n",
    "    for attn in layer_attn_list:\n",
    "        if attn.shape[-2] == 1:\n",
    "            num_generated_tokens += 1\n",
    "        curr_kv_len = attn.shape[-1]\n",
    "        final_attn.append(F.pad(attn, (0, max_len - curr_kv_len, 0, 0, 0, 0, 0, 0)))\n",
    "    \n",
    "    # print(num_generated_tokens)\n",
    "    return torch.cat(final_attn, dim=-2), num_generated_tokens\n",
    "\n",
    "\n",
    "\n",
    "def combine_all_layers(attns):\n",
    "    for key in attns.keys():\n",
    "        if key.startswith(\"llama\"):\n",
    "            attns[key] = combine_attention(attns[key])\n",
    "    return attns\n",
    "\n",
    "\n",
    "def rollout(attentions, discard_ratio=0.8, head_fusion=\"min\"):\n",
    "    result = torch.eye(attentions[0].size(-1))\n",
    "    with torch.no_grad():\n",
    "        for attention in attentions:\n",
    "            if head_fusion == \"mean\":\n",
    "                attention_heads_fused = attention.mean(axis=1)\n",
    "            elif head_fusion == \"max\":\n",
    "                attention_heads_fused = attention.max(axis=1)[0]\n",
    "            elif head_fusion == \"min\":\n",
    "                attention_heads_fused = attention.min(axis=1)[0]\n",
    "            else:\n",
    "                raise \"Attention head fusion type Not supported\"\n",
    "\n",
    "            # Drop the lowest attentions, but\n",
    "            # don't drop the class token\n",
    "            flat = attention_heads_fused.view(attention_heads_fused.size(0), -1)\n",
    "            _, indices = flat.topk(int(flat.size(-1)*discard_ratio), -1, False)\n",
    "            indices = indices[indices != 0]\n",
    "            flat[0, indices] = 0\n",
    "\n",
    "            I = torch.eye(attention_heads_fused.size(-1))\n",
    "            a = (attention_heads_fused + 1.0*I)/2\n",
    "            a = a / a.sum(dim=-1)\n",
    "\n",
    "            result = torch.matmul(a, result)\n",
    "    \n",
    "    # Look at the total attention between the class token,\n",
    "    # and the image patches\n",
    "    mask = result[0, 0 , 1 :]\n",
    "    # In case of 224x224 image, this brings us from 196 to 14\n",
    "    width = int(mask.size(-1)**0.5)\n",
    "    mask = mask.reshape(width, width).numpy()\n",
    "    mask = mask / np.max(mask)\n",
    "    return mask    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attns = torch.load(\"debug.pt\")\n",
    "# modalities = torch.load(\"debug_modalities.pt\")\n",
    "# for layer_idx in range(1):\n",
    "#     combined_attn = combine_attention(attns[f\"llama_attn_{layer_idx}\"])\n",
    "#     img_mask, vid_mask, text_mask = calculate_modality_indices(modalities, seq_len=combined_attn.shape[-1])\n",
    "#     pooled_combined_attn = torch.mean(combined_attn, dim=1)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(0.0431)\n",
      "tensor(0.)\n",
      "tensor(0.1255)\n",
      "tensor(0.)\n",
      "tensor(0.0902)\n",
      "tensor(0.)\n",
      "tensor(0.0431)\n",
      "tensor(0.)\n",
      "tensor(0.0745)\n",
      "tensor(0.)\n",
      "tensor(0.0706)\n",
      "tensor(0.)\n",
      "tensor(0.0588)\n",
      "tensor(0.)\n",
      "tensor(0.0431)\n",
      "tensor(0.)\n",
      "tensor(0.0627)\n",
      "tensor(0.)\n",
      "tensor(0.1020)\n",
      "tensor(0.)\n",
      "tensor(0.0824)\n",
      "tensor(0.)\n",
      "tensor(0.0431)\n",
      "tensor(0.)\n",
      "tensor(0.0627)\n",
      "tensor(0.)\n",
      "tensor(0.1059)\n",
      "tensor(0.)\n",
      "tensor(0.0745)\n",
      "tensor(0.)\n",
      "tensor(0.0784)\n",
      "tensor(0.)\n",
      "tensor(0.1255)\n",
      "tensor(0.)\n",
      "tensor(0.1216)\n",
      "tensor(0.)\n",
      "tensor(0.0784)\n",
      "tensor(0.)\n",
      "tensor(0.0745)\n",
      "tensor(0.)\n",
      "tensor(0.1294)\n",
      "tensor(0.)\n",
      "tensor(0.0706)\n",
      "tensor(0.)\n",
      "tensor(0.1176)\n",
      "tensor(0.)\n",
      "tensor(0.1373)\n",
      "tensor(0.)\n",
      "tensor(0.0471)\n",
      "tensor(0.)\n",
      "tensor(0.0588)\n",
      "tensor(0.)\n",
      "tensor(0.1098)\n",
      "tensor(0.)\n",
      "tensor(0.1059)\n",
      "tensor(0.)\n",
      "tensor(0.0902)\n",
      "tensor(0.)\n",
      "tensor(0.0824)\n",
      "tensor(0.)\n",
      "tensor(0.1020)\n",
      "tensor(0.)\n",
      "tensor(0.1765)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAGFCAYAAAA1jW6gAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcuUlEQVR4nO3dSWxd13nA8XPfI/mogZNNUQxNmfGQ1CgEQ41leRDaogZauCgCr4wi3rTdeFF4X3RVdFV0WSCbeJMCTb2oi7RKgMBFEAVo40mWU1dRXVmxoshiaImUJQ6Sxem924Vs8n7fI895h/e7b/z/Vu/4TucN1Odzv3u+k6RpmjoAAAyUWt0BAED3IKgAAMwQVAAAZggqAAAzBBUAgBmCCgDADEEFAGCmr5GdarWam5ubc0NDQy5JkqL7BABoI2maupWVFTc1NeVKJf9YpKGgMjc3544cOWLSOQBAZ7p69aqbnp727tNQUBkaGnLOOff7I3/q+pIB55xztbtrYp/Svopop9OTW6+vnxwT2w59591GLgsAaAObbsP9zP1oKxb4NBRUvrzl1ZcMbAeVpCb2KX3x37+UlreDTHlgUF406W/ksgCAdvBFMa9G0h8k6gEAZhoaqeR1+J1l0V566WnRHn7tnWZ0AwBQMEYqAAAzBBUAgBmCCgDATFNyKuXZBdGuHpWPpaUnj4l28uYHBfcIAFAERioAADMEFQCAGYIKAMBMVE4lqQy6pPTFzPm7qw0fl67KfQ/9dFa0F09MifYBciwA0JEYqQAAzBBUAABmom5/pWurLlWFJBuRDA56tw9dviPa5YUl0V588SnRPvg6VY4BoB0xUgEAmCGoAADMEFQAAGYKK9OSDmyfWj9SXLfMyy9+KZpVtXn0jGxfe/mZrdfjr769xx4CAKwxUgEAmCGoAADMEFQAAGYKy6kk65vbrwPzVELSJbkc8eaB7azM5nNPiG19p9/PdS0AwN4xUgEAmCGoAADMEFQAAGaaUvq+emvRu708Nhq1/9RPbm69Li3JumHZOSzOMY8FAJqJkQoAwAxBBQBghqACADDTlPVUQnQOpbTPP68lm0fxzWFxzrnS44+Jdu3chT30EADQCEYqAAAzBBUAgBmCCgDATGG1v7LStbWo/UNZm2wkrKn5MtPfvyqvXRkQ7dusdw8AhWGkAgAwQ1ABAJghqAAAzDSl9lcsnYNJKpWGj62NHBBtXRts6LJsl2eOiPbmFZmTAQA0jpEKAMAMQQUAYKYpjxRbE8sTq9twpflbop2uqu3qEWO3ti6ad184Idr7Tp3ZYy8BoPcwUgEAmCGoAADMEFQAAGYKy6kkqxtm59KPGIeWG87SZVxK8zfkudX+s8/JOPvojWNbr5M3P2j4ugDQixipAADMEFQAAGYIKgAAM4XlVNLBfrNz6TIt2eWGdc5Ez0upO9egf6nih07JXFD/7Gdbr6vHj8prnT3vPRcA9BpGKgAAMwQVAIAZggoAwExb1v4KlbrXeRRxrMqZ6KgZyrlUZhfl/kvL2+dSdcM2Tx6T12YeC4Aex0gFAGCGoAIAMENQAQCY2XNOJTtXxFpoOWG9XWxTOZPq4pL3XJquDZbN35SXb4tt+sO7zVosAHocIxUAgBmCCgDADEEFAGAmKqeSrq26NKk553ZYp6SJOZasUI4k+tzqfWTfV90cF9U+oOqdVZnHAqDHMFIBAJghqAAAzBBUAABmOmKNeh+dE6lF7h+SnefSN3lYbru1KNob4wdEe/V++fEOP/6YaNfOXYjqCwC0O0YqAAAzBBUAgJmo219JZdAlpS/Kv3vKzztnu5xwfT+2HyOOvZ0VU/Klbn9V+r48OSF3XpbnGvxw1nvulOWJAXQZRioAADMEFQCAGYIKAMBMU5YTDpVSCZW695WA0Y8Q631jt+vliH25o1TlWErLd0VbP3KsJRNjsi+Zsi6UdAHQiRipAADMEFQAAGYIKgAAM3sufW8plHPRZfbLY6Nbr3VUTEaGRTs2ataVt89uW1qW11L5F9+xztXnjkqffCra/ZnzbVA2H0AHYqQCADBDUAEAmCGoAADMNGWeSlR9rQZk53/oc5f1uSPzHjp/kz1/3ZyXiGMbuVZ2Tkz/Jblp9bknRLvv9PvecwNAKzBSAQCYIagAAMwQVAAAZpqSUwkJ1f7KQ9ff8tUR22l7NdO3urphKifiOzaWzv2sPCjrjI1//RF5rYsqCQMALcBIBQBghqACADBDUAEAmClsjfo8YtedzwrNFQnlOfT2bH6nbl5J4NqWDr3xK/kf1Fou8688K9oT336rsL4AwG4YqQAAzBBUAABmzErf1z1eu7qx9161UMzjzHlu04WOr/uUA7fWBm8+IM9N6XwALcBIBQBghqACADBDUAEAmGmLMi3IrzqQiPbKjCwZs7+yXTqfsvkAisJIBQBghqACADBDUAEAmDHLqeiy7+lg/6776rkgscsN550fUpTQHBfrZZWzRn95V7Q3huTn37+cWRaZOSwACsJIBQBghqACADBDUAEAmGnJPJXYnEi75lC0vLmhbFvvG1rKuG9hRbXltdemR7deV2YXZb8mD4v25rXrDgD2gpEKAMAMQQUAYIagAgAwU1hOJenQ9VR8ymOjop0391AeHRHt6uLS1mudb9Hrq9TlY9S8oNLSHdHO5lFqw/vEtrWH7xPtwbkxee1zFxwANIKRCgDADEEFAGCGoAIAMBOVU0kqgy4pDdxrBNZM99X+yis7h8N6Dos+X/Za1VuLu25rpC+hPInOsYh91ecdrDO2tLz7xuEjoln56Tl5rNo9OX5Ubj973nttAL2LkQoAwAxBBQBghqACADATlVNJ11ZdmuhMwC77DmyfWucKdH6g/jq75zWck/NF0lX/uULzO2LWQMmz3kkjsp9LaWZabCvN39h1X+ecSz+6LLd73mdsTqT88VXRvvHnz4j22D++HXU+AN2LkQoAwAxBBQBgxqxMS90jr+ubu+4bKuMeus2UfbRXnysZGRbtNFP6ZCd5HknO+ziz7/jaldmoY/VnludWnT5Wfz+fH05Ee/CFE6K979SZPV8bQGdjpAIAMENQAQCYIagAAMw0ZTnhqsprxDzGG9pfn7ustoceZ47Ji7TTssaxj2nHCH3+D/6bLPmfrK2L9s2Xnt56PfzaO2b9AtD+GKkAAMwQVAAAZggqAAAzcTmV8fudK9+7v67Lhmi6fLrYFpmbiNlf51jama90fug9VwN5jyLzP3oOjZ4rdO13v7L1evAvZsS2gT+8Uli/ALQeIxUAgBmCCgDADEEFAGAmLqdy4zPnknvLCYfmRZRWN/bcqTz5gby5hWbmJizPrfMaOudiSZflT5Zvi/bXvrc9b+WTP3pAbHvw5Jg89s0PbDsHoKUYqQAAzBBUAABmCCoAADNNqf0VoutYhWqF+XIRobpVoTxGO9X3ygrVS7Os/aXVfaaq1pde0rl/9rOt16WN/btuc07WCXOOWmFAp2OkAgAwQ1ABAJghqAAAzLRFTkXnUPomD8vtmTXpQ5o5z6SZQu8jlHPJbs89d2dpWbR1Pqc8uD1npv+OPJc+1jk5jyU9eUxem3ksQEdhpAIAMENQAQCYIagAAMwUllPRcxlibF67Ht5pF92SQ4lV5PwbfWwtsH82Bzb1k5ty48S4aN53+rJoT/67TML85pWjsi9nzweuDqCVGKkAAMwQVAAAZgq7/ZVWBvZ8bOjx2F69xdUq+vvQZfZ9JWKu/vF9ov3gd/5XtFN1O2zuZVkavzy/INqLLz4l2gdff3fXawNoPkYqAAAzBBUAgBmCCgDATFuUadHy5ExCZfTzsC4BE3O+Tio/k825VG6lYpvOv5Tmb4h2qrZX1blH/+vXoj33yrNbrye+/VZkTwFYY6QCADBDUAEAmCGoAADMtGVOJY88SxGHWOcxYs4XWiY5z7ljr+3UPBXf9rWxJOpa5bFR0Q4tezD5n9tlYNafe0Js6zv9ftS1AeTHSAUAYIagAgAwQ1ABAJjpupxKJ83nyCM2x2LJV+tL0/NUgufSSxNPToi2Xo44Wd3Yer16v/w5Dz/+mLzWuQvevgDIj5EKAMAMQQUAYIagAgAw03U5lWbW48LOYnIuIdVr86Kt13LJLls9/JG/zlt68pg89s0PcvUNQD1GKgAAMwQVAIAZggoAwEzX5VTy6tQcSjP7rfMaVXXtbF907a/YfuocV916LNl91RwWNzEumgu/d59oH3LH5LXIsQC5MVIBAJghqAAAzBBUAABmyKkonTpPpZ36ne1Led2zoyu238nybdE+9N9Dot1/6VN57a8/ItrVi5fM+gL0CkYqAAAzBBUAgBlufym+2y/tdItJK7IvobL6rfxcksHtx5vrlh5Wjx/3LRwU7XRVbtcLH9994cTW632nzuy5j0AvYaQCADBDUAEAmCGoAADMkFNBbr6ljfvu+JcTDuVfQssm1+VRfOZviKav5Itzzi09vP3ncYCliYGGMFIBAJghqAAAzBBUAABmyKlEaKd5Kc1U975V6Xvf/ncn9OwPKTTHRW/XZfezQssYZ+e03Ovcqnf7+P9s96U0f0tsq7I0MbAjRioAADMEFQCAGYIKAMAMORUUqrSR7/hgPie7ZPCVWe+5QnNa9PbK7MFdt+k6Yneff1K0B954z3stoFsxUgEAmCGoAADMEFQAAGbIqbSp8uiIaFcXl1rUk/q5Ino+R1LZfX5IaDnhEP056Gv7K4v5hebE6FphYl+1VHFpfVS0b7/4lGgffP3d+A4CHYiRCgDADEEFAGCGoAIAMNN1OZV2Xkc+hq5j1cr3FXutbH2ujQNyW+x696E1T5LKwNbr8tio2KbXoNeqgfeVvbb+DPS8lcEP5bG6Qln1+FHZt7PnvdcGOhUjFQCAGYIKAMAMQQUAYKbrciqhuQedkmMJ9TOUmyjyfepchb5WLfu633n3jVkvxbkd1kzJtEPHhubXBOet+GRyO845ly4ti3Z5Qc4z2mA9FnQpRioAADMEFQCAma67/aV1yu2uWLlu1eQUWrY327e+z+POrcvRlL/+iNzBU94+1C+9fLBm+RmGHoW+c2JKtEcy77N68ZJZP4BmY6QCADBDUAEAmCGoAADMdH1OpVe1y6PUeUvf1wJLBGfFPk6e5zOqK9tybd67v36ceeTHF+T2keGt18svPS22Db/2TsP9AlqNkQoAwAxBBQBghqACADBDTqVLxJR1aWZ+RZe+1/L2xXd86Nx5S+HE0KXytez/3ZU25ByWlJIu6CCMVAAAZggqAAAzBBUAgBlyKj3CMo+iS8z7luWtLOa7VitrnOXpR+ycmOxyAeX1VGwrr8g6Ypf+7hnRfuiv3/b2BWgmRioAADMEFQCAGYIKAMAMORVEC65bkt23P7yPTyh3UeT8m5jz6TxTbZf9vuR7Xwcuy6WIq0Py3I9+V9YZu/3CCdHed+pM4OpAcRipAADMEFQAAGYIKgAAM+RUEKTv/8fMU6kO5Lt2TF6jyDVkyqMjoq3zSjF5Jud2yMFkji8t3ZH7qvbqIxOirXMwpZkjor155WpU34A8GKkAAMwQVAAAZggqAAAz5FQitMu6750k7xr1MYr8PmJzJqG++PJQdftek/NSytP3y+1qHsv6uFzEplKRia3qxUsNXxuIxUgFAGCGoAIAMMPtrwi9erur7n2rx2F91kbzXTum1H2R30+olL1+RFh/RqHbZ+L8a/KeoT53efYz0daPGFd+/rH3Wsnxo9vXPXveuy8Qi5EKAMAMQQUAYIagAgAw05ScivWjuL5y56187Nfy2rHL1eoyItXFpT1fOyQZGZbXVttFyZENdWzk+4rJZehy883MsYRK3ZfHRkW7emtx1331trr3ob7bPlWGpRroS/KL7e/n9otPiW0HX383cDTgx0gFAGCGoAIAMENQAQCYack8lby5h+z+ecqyW7O8hx97riJzKFq6tOzdLvIcBS8nnM3ftHIeUbAsSyhPEsEyd3d7WmbEBp5/UrbfeG/P50ZvYqQCADBDUAEAmCGoAADMNCWnEpprEFPfSZ9Pn7uZOZRepetYxdzTL/K3oMXOiSlS6Fqx7zuPbF+mvy/nuOh82Sd/9axoP/D3bxXXMXQFRioAADMEFQCAGYIKAMBMYTmVZG33dWR7dV2SbhHz/X31h/IefZrjXLE66XeWJy+V67oqh6LzZauHZFWz0uOPyf3PXTDrC7oDIxUAgBmCCgDADEEFAGAmKqdSu7vmaklo5Yh70srAnjrkXL7aRq1cT6VXxHzGv/6mXHtl5mzctfomD4u2ZQ2tTmW6bs+grJWn/y9z5kdyQZzS/C3RvvPCCdHed+rMnvuC7sBIBQBghqACADBDUAEAmGnJeipFauV6KrjHtI5Vjtwc4ul5KoOX5uX2iTHRvnNYrsey//jRrdfp2fPGvUMnYKQCADBDUAEAmIm6/VXaV3Gl5N7tCD1M1nxlWkLyPCLZzGV1e1Xo+8luD5VpCdm8cjW8U48xLdOyKv+O624fX5O3v9Ir8tqT8/KR75WnZrZe73dHxTZuh/UGRioAADMEFQCAGYIKAMBMYY8U5ynT0is6taRMTL8XvjEk2uORZVo6RSd9l9m+6jItugyOzrE43VZ/50MffLr1+ubJB8S29W88I9rjr77dSHfRYRipAADMEFQAAGYIKgAAMy1ZThj3tPN9d5+Yfpdz/gw6JVfRrv3aSbavoaUE9EIXOseilyPOWnr4iGjP/OCmaFM2vzsxUgEAmCGoAADMEFQAAGaYp4JCVXP+DDopV9GJymOjor157bpo181TUXzzXB76J1m3bfWRCdHe/5vPRXvjuSdEu+/0+95roz0xUgEAmCGoAADMEFQAAGa6bjnhTpnX0CsGbseuoIJm0vNUNL1ukv57Siq7b68eGhHbBj+clfuqtVwqE+OivXnymLzWmx94+4r2wEgFAGCGoAIAMENQAQCY6bqcCjmU9lLaIKfSyfL8PdUq8p+Xksqh1O0/vE+0N4b6RXv/jKwltnlFzoNBe2CkAgAwQ1ABAJjputtfaC+1/qTVXYCHLsNSrXtkeO+P6PctrMT1ZfmuaO+fXRDt2sSYaF/522e3Xs/8zVtR10JxGKkAAMwQVAAAZggqAAAz5FRQqM8Py/9vGW5RP2CjPCpLr+gyLlnJ8m35H0bUt6+WHA89fJ588qloP/zPG1uvb730tNg2/No7gbOhKIxUAABmCCoAADMEFQCAmcJyKom6X4reVNoI7xOjU5c2aKd+Z/uilwN2bkm09HLDnx97ULT3vXNRtGuZ17q0fS1QZr+s2nppY/0ZuivbpfT3T8t+zr/yrGhPfJt5LM3CSAUAYIagAgAwQ1ABAJgpLKeSVgaKOrVXO9277lYxn3Gtf9dNe9Kp32f9Mryt+536rlWXt1AqN+56t2f55rDsRC9tHOpLll6q+CuzB0X79gsnRHvfqTNRfUPjGKkAAMwQVAAAZggqAAAzXVf7q1PvuXeSmM84dp5Kr+TE2uV96bkkoX5df1rW75qalfNc0sXteS4xdcKc22Ftl8WlXfa8J/tb0e8jUTndg//3mWjfff5J0R544z3vtdA4RioAADMEFQCAGYIKAMBMW9b+6pX76qinv+u+ycOirecy9OJvo5l/HzpXMfXDq6Ktvw9x7INfkf/ho8tW3aqj8zX6/5YTtZbL+pDco/TcE6Ldd/p9s771GkYqAAAzBBUAgBmCCgDATFvW/urF++TdavTjfAuq6DU1YPv3EZo7UjdXJDB3JCtVOZRQv6tF5oaWlkV75MeyXXv0iGgvvPyMaI+/+nYxHetCjFQAAGYIKgAAM235SDG6R/+K8XrC6BhF38b2Lrmg2roEjL7tV55dEO33f/A90f6T//jm1uvNK/KxakiMVAAAZggqAAAzBBUAgJm2fKQY3aNaKYt21621gLagS9foHEoyKNtl1a5OHxLto//wl6I9s3Zp6/U6ZfO9GKkAAMwQVAAAZggqAAAzUbe4a3fXXC3RT4AXj1L4nWvxUZlbGz+d73z8FmzVLeGrPk/9eWu+zz+U5wiViNHn9vVFn7vuXMMH5bnUPLrygiw/M/Uz+U9jdgmAwbkVsW2VHIvASAUAYIagAgAwQ1ABAJiJyqmU9lVcKbl3jzx0P9QS9807V20gMT0fvwVbsX/H5ckJ0Y6pg1VXRl+Jzd9k969bTljnWOZvyHN5r+Rcn2eeXWnpjmjrbE56/Khsnz0fuFp3YaQCADBDUAEAmCGoAADMdF0pJuYxFC/mM671F90b5BGap6KFcijZ30Z5bFQeq5aGDuVQfOd2TvZd1/aq3lrcdd+d1OVkVA4mmw9K9L6VaXXsLdH+vMfmsTBSAQCYIagAAMwQVAAAZjqi9leM2HvEiBfKU2XvfW8cKLo3yMN6vln2t7H62zLX0KdyKvp3VB4dEW39t1v3u8v8rWdrc+0k9n3qHI1znjk2Ov+irnX9+COi/dVfyXb14iXXTRipAADMEFQAAGYIKgAAMx1R+ytGqL4Qipe9913aaGFHkJvOY/TNHBFt37yVwUvzct/AtWL/TSny3yBfjiY050Vv/+q/yFySzsFk17zvhjksjFQAAGYIKgAAM1G3v5LhIZeUvnhctE1vf6G9bO5vdQ/gE1vGKKrU/bX58E45+pKnBFOoRIy+teYrsx9c9vjKrPdaV75V3W586xti29f+7OfeY9sRIxUAgBmCCgDADEEFAGAmKqdSvX7DJUljtcyTtfU9dQjd5cBvQgu3+rGUQbFiP9+Y/df+4HHRDj0u2zd5WLR1qfxQX2KE8iClGVliJltKRe8b6kfyWw/Ja390WbQffXW79FW1Upb7njwmz/XmB95rtQNGKgAAMwQVAIAZggoAwExhZVrSysDee4WuURtIch1PDqVzVX56Tv0Hf+5BLwEckv1t5F2aWP/OaoG5JVmhsi3uk0/ltdTmvoWVrdcbD98ntg1+KPux+OJTon3w9Xcb62QTMVIBAJghqAAAzBBUAABmonIqAHpbzHLddfsGlqXIM+9Ea2YuLvS+9DLJdXNkMnP6+lfkWhG6BP+Bub0vRdAsjFQAAGYIKgAAMwQVAIAZcioo1ODNWngndIyY5bpjl/vNkwexzqH4zhd7rdBnli4tb70uHRrx7Olc/+xnon3z5AOiPaKOT8+eb6SLphipAADMEFQAAGYIKgAAM+RUUKhqztpfKFbMvJNm69S1dELzbXzvo1ZR/ySrvFRpYly0Ry7eltde35THHz8qr92EHAsjFQCAGYIKAMAMQQUAYIacCtBhLHMNsXNJYuTN18S8r3bKv4Surfua/Q5Ka5t6d6/Sx7LWl/4+y2Ojor2RWfO+qPXuGakAAMwQVAAAZgq7/ZUt5wwAIe10C6tI+n1lS+Proka+MvnOOVcN3L6sTYyJ9sLv7N96PXXpsNi2ee2691yNYqQCADBDUAEAmCGoAADMFJZTSSsDRZ0aHaTKz8BcM3MNefIcRT6uHJKnVIr1tUKyn9PGsDxXv378eOSAPPia/9yl+VuiPfWv2229VPH680+K9sAb7/lPvts193QUAAA7IKgAAMwQVAAAZjqyTEv2Hmbdc9w98qy71sz3nX2u3rm4JWat+X4L7aRdf5flyQnRzi5tuxNdasX3vmLfo+VywrpfumRMosqXVG8tmvUlxHfu/mW5Te+bnrsQda2YuSc6h3Lj5We2XlfXV5377qmGzsNIBQBghqACADDT0O2vNE2dc85tptslAmrphveYUnV72FYN7BsrSbdjYarOnd220/Zu1cz3naaqVITnWtV1+djiZhN/C+2kbX+XNXV7JfWXV9Lfdbu+L92vkmonNdnW7yvmfehrhfjOnVTl30srP8/s3+6Xr7+MBT5J2sBes7Oz7siRIzm6BwDodFevXnXT09PefRoKKrVazc3NzbmhoSGXJKw5DgC9JE1Tt7Ky4qamplyp5B+ZNRRUAABoBIl6AIAZggoAwAxBBQBghqACADBDUAEAmCGoAADMEFQAAGb+H9gcGToSTtcmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attns = torch.load(\"video_vanilla_debug.pt\")\n",
    "modalities = torch.load(\"video_vanilla_debug_modalities.pt\")\n",
    "for layer_idx in range(32):\n",
    "    combined_attn, num_generated_tokens = combine_attention(attns[f\"llama_attn_{layer_idx}\"])\n",
    "    img_mask, vid_mask, text_mask = calculate_modality_indices(modalities, seq_len=combined_attn.shape[-1])\n",
    "    pooled_combined_attn = torch.mean(combined_attn, dim=1)\n",
    "    \n",
    "    img_attn_wrt_gen_tokens = pooled_combined_attn[..., -num_generated_tokens:, vid_mask[0].to(torch.bool)].cpu()\n",
    "    text_attn_wrt_gen_tokens = pooled_combined_attn[..., -num_generated_tokens:, -num_generated_tokens:].cpu()\n",
    "\n",
    "    num_tokens = 4\n",
    "    step = 2048 // num_tokens\n",
    "    avg_img_attn = torch.nn.functional.avg_pool1d(img_attn_wrt_gen_tokens.float(), kernel_size=step, stride=step) * (256 / num_tokens)\n",
    "    \n",
    "\n",
    "    \n",
    "    pooled_combined_attn = torch.concat([avg_img_attn, text_attn_wrt_gen_tokens], dim=-1)\n",
    "    pooled_combined_attn = pooled_combined_attn.permute(1,2,0).cpu().numpy()\n",
    "    \n",
    "    score_map = torch.tensor(np.uint8(255 * pooled_combined_attn)).to(torch.float32).permute(2, 0, 1) / 255.\n",
    "    print(torch.min(score_map))\n",
    "    print(torch.max(score_map))\n",
    "    # plt.plot(score_map.numpy()[0])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(score_map.permute(1,2 ,0).numpy(), cmap='viridis', interpolation='nearest')\n",
    "    os.makedirs(\"vanilla/video/\", exist_ok=True)\n",
    "    plt.savefig(f\"vanilla/video/vanilla_video_attn_wrt_gen_tokens_{layer_idx}.png\",  bbox_inches='tight', pad_inches=0.0)\n",
    "    # cv2.imwrite(score_map, )\n",
    "    # save_image(score_map, f\"img_attn_wrt_gen_tokens_{layer_idx}.jpg\")\n",
    "    \n",
    "    \n",
    "    # print(img_attn.shape)\n",
    "    # num_tokens = 8 # MUST DIVIDE 256\n",
    "    # step = 256 // num_tokens\n",
    "    # # avg_img_attn = torch.nn.functional.avg_pool1d(img_attn, kernel_size=step, stride=step)\n",
    "    # avg_img_attn = torch.zeros(1, img_attn.shape[-2], num_tokens)\n",
    "    # for idx in range(256):\n",
    "    #     avg_img_attn[..., idx % num_tokens] += img_attn[..., idx]\n",
    "    # print(text_attn.shape)\n",
    "    # avg_img_attn = avg_img_attn[..., img_mask[0].to(torch.bool), :].permute(0,2,1)\n",
    "    # avg_img_attn = torch.nn.functional.max_pool1d(img_attn, kernel_size=step, stride=step).permute(0, 2, 1)\n",
    "    \n",
    "    # print(avg_img_attn.shape)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_combined_attn[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(pooled_combined_attn, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
